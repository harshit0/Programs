{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Getting_Started_with_NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshit0/Programs/blob/master/Getting_Started_with_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ToVaLXIdopY",
        "colab_type": "text"
      },
      "source": [
        "## Getting Started with Neural Networks: Neural Network for Logic Gates\n",
        "\n",
        "![XOR Truth Table](https://www.researchgate.net/profile/A_Zahedi/publication/321687478/figure/fig2/AS:602499278979075@1520658430966/a-circuit-symbol-and-b-accuracy-table-of-the-XOR-logic-gate.png)\n",
        "\n",
        "\n",
        "((x1    , x2    ), y)\n",
        "\n",
        "**((input1, input2), output)** -- Let's use this convention\n",
        "\n",
        "Training Set: [((0, 0), 0), ((0, 1), 1), ((1, 0), 1), ((1, 1), 0)]\n",
        "\n",
        "Therefore, X = [[0, 0], [0, 1], [1, 0], [1, 1]] and Y = [[0, 1, 1, 0]]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQlJHGoMiEf2",
        "colab_type": "text"
      },
      "source": [
        "We are going to use this NN. Activation function for this Sigmoid Function : ![alt text](https://miro.medium.com/max/273/1*m6Bs9A8FvWbqCa7nqn_GWg.png)\n",
        "![alt text](https://miro.medium.com/max/960/1*R4twuYNUKXVzsvgOGkPZsA.png)\n",
        "\n",
        "![Single Layer NN](https://miro.medium.com/max/815/1*qXt_iBvWods-FOvTldxYFw.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPRLhcWs-ZS9",
        "colab_type": "code",
        "outputId": "6df1218f-289a-4987-82ca-2769160ac0fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "\n",
        "import numpy as np \n",
        "#np.random.seed(0)\n",
        "\n",
        "def sigmoid (x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "#Input datasets\n",
        "inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "expected_output = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "epochs = 10000\n",
        "lr = 0.1\n",
        "inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2,2,1\n",
        "\n",
        "#Random weights and bias initialization\n",
        "hidden_weights = np.random.uniform(size=(inputLayerNeurons,hiddenLayerNeurons))\n",
        "hidden_bias =np.random.uniform(size=(1,hiddenLayerNeurons))\n",
        "output_weights = np.random.uniform(size=(hiddenLayerNeurons,outputLayerNeurons))\n",
        "output_bias = np.random.uniform(size=(1,outputLayerNeurons))\n",
        "\n",
        "print(\"Initial hidden weights: \",end='')\n",
        "print(*hidden_weights)\n",
        "print(\"Initial hidden biases: \",end='')\n",
        "print(*hidden_bias)\n",
        "print(\"Initial output weights: \",end='')\n",
        "print(*output_weights)\n",
        "print(\"Initial output biases: \",end='')\n",
        "print(*output_bias)\n",
        "\n",
        "\n",
        "#Training algorithm\n",
        "for _ in range(epochs):\n",
        "\t#Forward Propagation\n",
        "\thidden_layer_activation = np.dot(inputs,hidden_weights)\n",
        "\thidden_layer_activation += hidden_bias\n",
        "\thidden_layer_output = sigmoid(hidden_layer_activation)\n",
        "\n",
        "\toutput_layer_activation = np.dot(hidden_layer_output,output_weights)\n",
        "\toutput_layer_activation += output_bias\n",
        "\tpredicted_output = sigmoid(output_layer_activation)\n",
        "\n",
        "\t#Backpropagation\n",
        "\terror = expected_output - predicted_output\n",
        "\td_predicted_output = error * sigmoid_derivative(predicted_output)\n",
        "\t\n",
        "\terror_hidden_layer = d_predicted_output.dot(output_weights.T)\n",
        "\td_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "\t#Updating Weights and Biases\n",
        "\toutput_weights += hidden_layer_output.T.dot(d_predicted_output) * lr\n",
        "\toutput_bias += np.sum(d_predicted_output,axis=0,keepdims=True) * lr\n",
        "\thidden_weights += inputs.T.dot(d_hidden_layer) * lr\n",
        "\thidden_bias += np.sum(d_hidden_layer,axis=0,keepdims=True) * lr\n",
        "\n",
        "print(\"Final hidden weights: \",end='')\n",
        "print(*hidden_weights)\n",
        "print(\"Final hidden bias: \",end='')\n",
        "print(*hidden_bias)\n",
        "print(\"Final output weights: \",end='')\n",
        "print(*output_weights)\n",
        "print(\"Final output bias: \",end='')\n",
        "print(*output_bias)\n",
        "\n",
        "print(\"\\nOutput from neural network after 10,000 epochs: \",end='')\n",
        "print(*predicted_output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial hidden weights: [0.95676772 0.39681058] [0.82646757 0.28103159]\n",
            "Initial hidden biases: [0.08763626 0.19069581]\n",
            "Initial output weights: [0.1968973] [0.87010705]\n",
            "Initial output biases: [0.78324778]\n",
            "Final hidden weights: [5.75202918 3.68571637] [5.74479219 3.6842491 ]\n",
            "Final hidden bias: [-2.38419465 -5.64042184]\n",
            "Final output weights: [7.39239718] [-7.99967945]\n",
            "Final output bias: [-3.33341642]\n",
            "\n",
            "Output from neural network after 10,000 epochs: [0.06077923] [0.94373811] [0.94376223] [0.06086699]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXbawZmWBLEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}